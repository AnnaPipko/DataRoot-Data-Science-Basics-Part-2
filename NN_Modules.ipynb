{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        return []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        if len(self.modules) == 0:\n",
    "            self.output = input\n",
    "        else:\n",
    "            y = self.modules[0].forward(input)\n",
    "            for m in self.modules[1:]:\n",
    "                y = m.forward(y)\n",
    "            self.output = y\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):       \n",
    "        if len(self.modules) == 0:\n",
    "            self.gradInput = input\n",
    "        else:\n",
    "            if len(self.modules) > 1:\n",
    "                g = self.modules[-1].backward(self.modules[-2].output, gradOutput)\n",
    "                for i in range(len(self.modules) - 2, 0, -1):\n",
    "                    g = self.modules[i].backward(self.modules[i-1].output, g)\n",
    "                g = self.modules[0].backward(input, g)\n",
    "            else:\n",
    "                g = self.modules[0].backward(input, gradOutput) \n",
    "            self.gradInput = g\n",
    "        \n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.zeros((input.shape[0], self.W.shape[0]))\n",
    "        for i in range(input.shape[0]):\n",
    "            self.output[i] = (self.W).dot(input[i].T) + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput.dot(self.W)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradW = gradOutput.T.dot(input)\n",
    "        self.gradb = np.sum(gradOutput, axis = 0)\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        \n",
    "        self.output = np.exp(self.output)\n",
    "        self.output = self.output / np.sum(self.output, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.zeros(input.shape)\n",
    "        for i in range(input.shape[0]):\n",
    "            g = np.zeros((input.shape[1], input.shape[1]))\n",
    "            for j in range(input.shape[1]):\n",
    "                for k in range(input.shape[1]):\n",
    "                    g[j][k] = - self.output[i][j] * self.output[i][k]\n",
    "                    if j == k:\n",
    "                        g[j][j] += self.output[i][j]\n",
    "            self.gradInput[i] = gradOutput[i].dot(g)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchMeanNormalization(Module):\n",
    "    def __init__(self):\n",
    "        super(BatchMeanNormalization, self).__init__()\n",
    "        self.mean = 0\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.mean = (self.mean + np.mean(input, axis = 0))/2.\n",
    "        if input.shape[0] == 1:\n",
    "            self.output = input - self.mean\n",
    "        else:\n",
    "            self.output = input - np.mean(input, axis = 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        if input.shape[0] == 1:\n",
    "            self.gradInput = gradOutput\n",
    "        else:\n",
    "            self.gradInput = gradOutput - np.mean(gradOutput, axis = 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BatchMeanNormalization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        if input.shape[0] == 1:\n",
    "            self.output = p * input\n",
    "        else:    \n",
    "            self.mask = bernoulli.rvs(self.p, size = input.shape)\n",
    "            self.output = input * self.mask\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * self.mask\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Dropout with p = %d\" %(self.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = input * (input > 0) + self.alpha * input * (input < 0)\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0) + self.alpha * np.multiply(gradOutput , input < 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(ELU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = input * (input > 0) + self.alpha * (np.exp(input) - 1) * (input < 0)\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0) + gradOutput * self.alpha * np.exp(input) * (input < 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ELU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftPlus(Module):\n",
    "    def __init__(self):\n",
    "        super(SoftPlus, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.log(1 + np.exp(input))\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * 1./ (1 + np.exp(-input))\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftPlus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Criterion:\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        return self.updateGradInput(input, target)\n",
    "    \n",
    "    def updateOutput(self, input, target):\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def updateOutput(self, input, target):   \n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def updateOutput(self, input, target):  \n",
    "        self.output = -1./ input.shape[0] * np.sum(target * np.log(input))\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput = -1./ input.shape[0] * target / input\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
